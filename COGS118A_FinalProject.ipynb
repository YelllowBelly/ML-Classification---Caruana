{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "physical-terror",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "moderate-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "import sklearn.metrics as sklm\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-candy",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "acting-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used same names as Caruana\n",
    "\n",
    "ADULT = pd.read_csv('adult.csv')\n",
    "COV_TYPE = pd.read_csv('covtype.csv')\n",
    "LETTER = pd.read_csv('letter-recognition.csv')\n",
    "KR = pd.read_csv('krkopt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-pierce",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "australian-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to transform letter data (Letter.p2 analog in paper)\n",
    "def letterClass(letter):\n",
    "    if(letter <= 'M'):\n",
    "        return 1\n",
    "    if(letter > 'M'):\n",
    "        return -1\n",
    "\n",
    "#function to transform  King Rook vs King dataset\n",
    "def notDraw(result):\n",
    "    if('draw' in result):\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "#fucntion to transform adult dataset\n",
    "def adultClass(income):\n",
    "    if('<=50K' in income):\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "#convert covertype to a binary classification problem\n",
    "def covertClass(class_):\n",
    "    largest_class = COV_TYPE['5'].max()\n",
    "    if(class_==largest_class):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "#Drop rows that have missing data\n",
    "def dropQuestion(data):\n",
    "    newArr = []\n",
    "    for i in data:\n",
    "        if type(data[i][0]) == str:\n",
    "            newArr.append(i)\n",
    "    for i in newArr:\n",
    "        data = data[data[i] != ' ?']\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "unavailable-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean adult data\n",
    "#ADULT = ADULT.iloc[:-20000] \n",
    "ADULT = dropQuestion(ADULT)\n",
    "ADULT[' <=50K'] = (ADULT[' <=50K']).apply(adultClass)\n",
    "ADULT=ADULT.drop(columns=[' Not-in-family',' Never-married', ' Adm-clerical'])\n",
    "#one-hot encode categories\n",
    "employment = pd.get_dummies(ADULT[' State-gov'])\n",
    "ADULT=ADULT.append(employment).drop(columns=[' State-gov'])\n",
    "\n",
    "location = pd.get_dummies(ADULT[' United-States'])\n",
    "ADULT=ADULT.append(location).drop(columns=[' United-States'])\n",
    "\n",
    "gender=pd.get_dummies(ADULT[' Male'])\n",
    "ADULT=ADULT.append(gender).drop(columns=[' Male'])\n",
    "\n",
    "race=pd.get_dummies(ADULT[' White'])\n",
    "ADULT=ADULT.append(race).drop(columns=[' White'])\n",
    "\n",
    "status=pd.get_dummies(ADULT[' Bachelors'])\n",
    "ADULT=ADULT.append(status).drop(columns=[' Bachelors'])\n",
    "\n",
    "#replace nan values with 0\n",
    "ADULT=ADULT.fillna(0)\n",
    "\n",
    "#Clean Letter data\n",
    "LETTER = dropQuestion(LETTER)\n",
    "LETTER['T'] = LETTER['T'].apply(letterClass)\n",
    "\n",
    "\n",
    "#Clean KR data\n",
    "KR=KR.rename(columns={\"a\": \"col1\", \"b\": \"col2\",\"c\":\"col3\"})\n",
    "KR['draw'] = KR['draw'].apply(notDraw)\n",
    "\n",
    "#one-hot encode categories\n",
    "col1 = pd.get_dummies(KR['col1'])\n",
    "KR=KR.append(col1).drop(columns=['col1'])\n",
    "col2 = pd.get_dummies(KR['col2'])\n",
    "KR=KR.append(col2).drop(columns=['col2'])\n",
    "col3 = pd.get_dummies(KR['col3'])\n",
    "KR=KR.append(col3).drop(columns=['col3'])\n",
    "KR=KR.fillna(0)\n",
    "\n",
    "#Clean Covertpye data\n",
    "COV_TYPE['5'] = COV_TYPE['5'].apply(covertClass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "specialized-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename Columns to use for classification\n",
    "ADULT=ADULT.rename({' <=50K':'classification'},axis='columns')\n",
    "LETTER=LETTER.rename({'T':'classification'},axis='columns')\n",
    "KR=KR.rename({'draw':'classification'},axis='columns')\n",
    "COV_TYPE=COV_TYPE.rename({'5':'classification'},axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-prediction",
   "metadata": {},
   "source": [
    "## Dictionary for Classifiers and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "protected-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier dictionaries with hyperparameters\n",
    "Classifiers = {\n",
    "    \n",
    "    'Random_Forest' : {\n",
    "        'name' : 'RandomForestClassifier()',\n",
    "        'hyperparameters' : {\n",
    "            'n_estimators' : [2**i for i in range(7)],\n",
    "            'criterion' : ['gini','entropy'],\n",
    "            'max_features' : [1,2,4,6,8,12,16,20]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'Logistic_Regression' : {\n",
    "        'name' : 'LogisticRegression()',\n",
    "        'hyperparameters' : {\n",
    "            'C' : [10**i for i in range(-4, 4)],\n",
    "            'max_iter' : [10000],\n",
    "            'penalty' : ['l1','l2'],\n",
    "            'solver' : ['sag','saga','liblinear']\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'Naive_Bayes' : {\n",
    "    'name' : 'BernoulliNB()',\n",
    "    'hyperparameters' : {\n",
    "        'alpha' : [10**i for i in range(-8, 4)],\n",
    "        'fit_prior' : [True,False]\n",
    "        \n",
    "        }\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-monday",
   "metadata": {},
   "source": [
    "## Training Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "upset-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore warnings using large datasets\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#use function so don't have to copy paste 60 times\n",
    "def train(clf, data,metrics):\n",
    "    \n",
    "    #5 trials for each dataset/classifier combination\n",
    "    for trial in range(5):\n",
    "        \n",
    "        #pick 5000 random samples/leftover as test samples\n",
    "        random_samples=data.sample(n=5000)\n",
    "        test_samples=data.drop(random_samples.index)\n",
    "\n",
    "        #format samples\n",
    "        train_y = random_samples['classification']\n",
    "        train_x = random_samples.drop(columns=['classification'])\n",
    "        X_train = train_x.values\n",
    "        Y_train = train_y.values\n",
    "        \n",
    "        test_y = test_samples['classification']\n",
    "        test_x = test_samples.drop(columns=['classification'])\n",
    "        X_test = test_x.values\n",
    "        Y_test = test_y.values\n",
    "        \n",
    "        #grid search hyperparameters\n",
    "        classifier = eval(clf['name'])\n",
    "        parameters = clf['hyperparameters']\n",
    "        search_results =  GridSearchCV(classifier, parameters, return_train_score=True,n_jobs=-1,refit=True)\n",
    "\n",
    "        #train data\n",
    "        search_results.fit(X_train,Y_train.reshape(-1))\n",
    "        \n",
    "        #error metrics per trial\n",
    "        Y_pred_train = search_results.predict(X_train)\n",
    "        Y_prob_train = search_results.predict_proba(X_train)\n",
    "        accuracy_train = search_results.score(X_train,Y_train)\n",
    "        f1_score_train = sklm.f1_score(Y_train,Y_pred_train,average='macro')\n",
    "        precision_train = sklm.precision_score(Y_train, Y_pred_train,average='macro')\n",
    "        \n",
    "        print('Trial ',trial+1,'params: ',search_results.best_params_)\n",
    "        print('Trial ',trial+1,': train accuracy: ', accuracy_train,', f score: ', \n",
    "              f1_score_train,', precision: ', precision_train)\n",
    "    \n",
    "    #error metrics with optimal hyperparameters\n",
    "    Y_pred_test = search_results.predict(X_test)\n",
    "    Y_prob_test = search_results.predict_proba(X_test)\n",
    "    accuracy_test = search_results.score(X_test,Y_test)\n",
    "    f1_score_test = sklm.f1_score(Y_test,Y_pred_test,average='macro')\n",
    "    precision_test = sklm.precision_score(Y_test, Y_pred_test,average='macro')\n",
    "\n",
    "    #append each trial metrics to list of 5 total\n",
    "    error_metrics = [accuracy_test,f1_score_test,precision_test]#,AUC]\n",
    "    metrics.append(error_metrics)\n",
    "    \n",
    "    print('Test Prams: ',search_results.best_params_)\n",
    "    print('Test accuracy: ', accuracy_test)\n",
    "    print('Test F score: ', f1_score_test)\n",
    "    print('Test Precision: ', precision_test)\n",
    "\n",
    "    \n",
    "    return metrics\n",
    "        \n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-eight",
   "metadata": {},
   "source": [
    "# Training Letter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-firmware",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "incorporate-spanish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  1 params:  {'criterion': 'entropy', 'max_features': 8, 'n_estimators': 64}\n",
      "Trial  1 : train accuracy:  1.0 , f score:  1.0 , precision:  1.0\n",
      "Trial  2 params:  {'criterion': 'gini', 'max_features': 4, 'n_estimators': 64}\n",
      "Trial  2 : train accuracy:  1.0 , f score:  1.0 , precision:  1.0\n",
      "Trial  3 params:  {'criterion': 'entropy', 'max_features': 6, 'n_estimators': 64}\n",
      "Trial  3 : train accuracy:  1.0 , f score:  1.0 , precision:  1.0\n",
      "Trial  4 params:  {'criterion': 'gini', 'max_features': 2, 'n_estimators': 64}\n",
      "Trial  4 : train accuracy:  1.0 , f score:  1.0 , precision:  1.0\n",
      "Trial  5 params:  {'criterion': 'entropy', 'max_features': 6, 'n_estimators': 64}\n",
      "Trial  5 : train accuracy:  1.0 , f score:  1.0 , precision:  1.0\n",
      "Test Prams:  {'criterion': 'entropy', 'max_features': 6, 'n_estimators': 64}\n",
      "Test accuracy:  0.9453296886459097\n",
      "Test F score:  0.9453273067858226\n",
      "Test Precision:  0.9453203204048627\n"
     ]
    }
   ],
   "source": [
    "letter_rf_trials_metrics = []\n",
    "letter_rf_trials = train(Classifiers['Random_Forest'],LETTER,letter_rf_trials_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-chrome",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hungry-supply",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  1 params:  {'C': 10, 'max_iter': 10000, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Trial  1 : train accuracy:  0.7396 , f score:  0.7395949585583976 , precision:  0.7396216537257341\n",
      "Trial  2 params:  {'C': 10, 'max_iter': 10000, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Trial  2 : train accuracy:  0.7268 , f score:  0.7267368652853556 , precision:  0.7271952450375687\n",
      "Trial  3 params:  {'C': 1, 'max_iter': 10000, 'penalty': 'l2', 'solver': 'sag'}\n",
      "Trial  3 : train accuracy:  0.7316 , f score:  0.7315090997208015 , precision:  0.7318591227274243\n",
      "Trial  4 params:  {'C': 1, 'max_iter': 10000, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Trial  4 : train accuracy:  0.7262 , f score:  0.7261281250651421 , precision:  0.7263158316119838\n",
      "Trial  5 params:  {'C': 100, 'max_iter': 10000, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Trial  5 : train accuracy:  0.7166 , f score:  0.7164500020510851 , precision:  0.7168269230769231\n",
      "Test Prams:  {'C': 100, 'max_iter': 10000, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Test accuracy:  0.7257150476698446\n",
      "Test F score:  0.7256369852520427\n",
      "Test Precision:  0.7263636370762012\n"
     ]
    }
   ],
   "source": [
    "letter_logreg_trials_metrics = []\n",
    "letter_logreg_trials = train(Classifiers['Logistic_Regression'],LETTER,letter_logreg_trials_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-church",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "surprising-conversion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  1 params:  {'alpha': 100, 'fit_prior': True}\n",
      "Trial  1 : train accuracy:  0.561 , f score:  0.4909312710831567 , precision:  0.6297393836229752\n",
      "Trial  2 params:  {'alpha': 1e-08, 'fit_prior': True}\n",
      "Trial  2 : train accuracy:  0.5544 , f score:  0.489229903580911 , precision:  0.6037355129800626\n",
      "Trial  3 params:  {'alpha': 1, 'fit_prior': False}\n",
      "Trial  3 : train accuracy:  0.5592 , f score:  0.5044464018761605 , precision:  0.6130762997547996\n",
      "Trial  4 params:  {'alpha': 1, 'fit_prior': True}\n",
      "Trial  4 : train accuracy:  0.5612 , f score:  0.49978331543436133 , precision:  0.6085847300040551\n",
      "Trial  5 params:  {'alpha': 1e-08, 'fit_prior': True}\n",
      "Trial  5 : train accuracy:  0.5606 , f score:  0.4952581142484752 , precision:  0.6178415868025064\n",
      "Test Prams:  {'alpha': 1e-08, 'fit_prior': True}\n",
      "Test accuracy:  0.5581705447029802\n",
      "Test F score:  0.492373359417939\n",
      "Test Precision:  0.614805133703111\n"
     ]
    }
   ],
   "source": [
    "letter_Bayes_trials_metrics = []\n",
    "letter_Bayes_trials = train(Classifiers['Naive_Bayes'],LETTER,letter_Bayes_trials_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-disposition",
   "metadata": {},
   "source": [
    "# Training Adult Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-consumer",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "structured-context",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  1 params:  {'criterion': 'gini', 'max_features': 16, 'n_estimators': 64}\n",
      "Trial  1 : train accuracy:  1.0 , f score:  1.0 , precision:  1.0\n",
      "Trial  2 params:  {'criterion': 'entropy', 'max_features': 12, 'n_estimators': 64}\n",
      "Trial  2 : train accuracy:  1.0 , f score:  1.0 , precision:  1.0\n",
      "Trial  3 params:  {'criterion': 'entropy', 'max_features': 8, 'n_estimators': 16}\n",
      "Trial  3 : train accuracy:  0.9998 , f score:  0.9932614889136628 , precision:  0.9967948717948718\n",
      "Trial  4 params:  {'criterion': 'gini', 'max_features': 12, 'n_estimators': 16}\n",
      "Trial  4 : train accuracy:  0.9998 , f score:  0.9941125541125541 , precision:  0.9971264367816092\n",
      "Trial  5 params:  {'criterion': 'gini', 'max_features': 6, 'n_estimators': 32}\n",
      "Trial  5 : train accuracy:  0.9998 , f score:  0.9947326116327194 , precision:  0.9969696969696971\n",
      "Test Prams:  {'criterion': 'gini', 'max_features': 6, 'n_estimators': 32}\n",
      "Test accuracy:  0.9936361066773078\n",
      "Test F score:  0.8048615920848802\n",
      "Test Precision:  0.8165103937009254\n"
     ]
    }
   ],
   "source": [
    "adult_rf_trials_metrics = []\n",
    "adult_rf_trials = train(Classifiers['Random_Forest'],ADULT,adult_rf_trials_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-hormone",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "antique-terminology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  1 params:  {'C': 0.0001, 'max_iter': 10000, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Trial  1 : train accuracy:  0.9942 , f score:  0.7517365115981033 , precision:  0.939203354297694\n",
      "Trial  2 params:  {'C': 0.0001, 'max_iter': 10000, 'penalty': 'l2', 'solver': 'sag'}\n",
      "Trial  2 : train accuracy:  0.996 , f score:  0.808641975308642 , precision:  0.9257154882154882\n",
      "Trial  3 params:  {'C': 0.0001, 'max_iter': 10000, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Trial  3 : train accuracy:  0.9948 , f score:  0.7631343617259111 , precision:  0.881045751633987\n",
      "Trial  4 params:  {'C': 0.0001, 'max_iter': 10000, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Trial  4 : train accuracy:  0.9938 , f score:  0.7119628339140535 , precision:  0.843190779496512\n",
      "Trial  5 params:  {'C': 0.0001, 'max_iter': 10000, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Trial  5 : train accuracy:  0.9918 , f score:  0.7074668653616022 , precision:  0.8454954954954955\n",
      "Test Prams:  {'C': 0.0001, 'max_iter': 10000, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Test accuracy:  0.9933848539415766\n",
      "Test F score:  0.7299584112531509\n",
      "Test Precision:  0.8468331639263749\n"
     ]
    }
   ],
   "source": [
    "adult_logreg_trials_metrics = []\n",
    "adult_logreg_trials = train(Classifiers['Logistic_Regression'],ADULT,adult_logreg_trials_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-nothing",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "distinct-sussex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  1 params:  {'alpha': 1e-08, 'fit_prior': False}\n",
      "Trial  1 : train accuracy:  0.9932 , f score:  0.7675213675213675 , precision:  0.8106657531117962\n",
      "Trial  2 params:  {'alpha': 1, 'fit_prior': True}\n",
      "Trial  2 : train accuracy:  0.9926 , f score:  0.6197718631178707 , precision:  0.5844444444444444\n",
      "Trial  3 params:  {'alpha': 1e-08, 'fit_prior': True}\n",
      "Trial  3 : train accuracy:  0.9914 , f score:  0.7381320820645935 , precision:  0.8040222507488233\n",
      "Trial  4 params:  {'alpha': 1e-08, 'fit_prior': False}\n",
      "Trial  4 : train accuracy:  0.9966 , f score:  0.8370433485994249 , precision:  0.8680624792289797\n",
      "Trial  5 params:  {'alpha': 1e-08, 'fit_prior': False}\n",
      "Trial  5 : train accuracy:  0.9928 , f score:  0.7709377236936291 , precision:  0.8428571428571429\n",
      "Test Prams:  {'alpha': 1e-08, 'fit_prior': False}\n",
      "Test accuracy:  0.9931548801483248\n",
      "Test F score:  0.7623172469546868\n",
      "Test Precision:  0.8017864626753672\n"
     ]
    }
   ],
   "source": [
    "adult_bayes_trials_metrics = []\n",
    "adult_bayes_trials = train(Classifiers['Naive_Bayes'],ADULT,adult_bayes_trials_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-pattern",
   "metadata": {},
   "source": [
    "# Training COV_TPYE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-september",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "successful-vietnamese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  1 params:  {'criterion': 'gini', 'max_features': 16, 'n_estimators': 64}\n",
      "Trial  1 : train accuracy:  0.9998 , f score:  0.9986492966212155 , precision:  0.999896006655574\n",
      "Trial  2 params:  {'criterion': 'entropy', 'max_features': 12, 'n_estimators': 64}\n",
      "Trial  2 : train accuracy:  1.0 , f score:  1.0 , precision:  1.0\n",
      "Trial  3 params:  {'criterion': 'gini', 'max_features': 6, 'n_estimators': 64}\n",
      "Trial  3 : train accuracy:  1.0 , f score:  1.0 , precision:  1.0\n",
      "Trial  4 params:  {'criterion': 'entropy', 'max_features': 20, 'n_estimators': 64}\n",
      "Trial  4 : train accuracy:  1.0 , f score:  1.0 , precision:  1.0\n",
      "Trial  5 params:  {'criterion': 'gini', 'max_features': 12, 'n_estimators': 64}\n",
      "Trial  5 : train accuracy:  0.9998 , f score:  0.9985072804411046 , precision:  0.9998964159933706\n",
      "Test Prams:  {'criterion': 'gini', 'max_features': 12, 'n_estimators': 64}\n",
      "Test accuracy:  0.9812816074693018\n",
      "Test F score:  0.837444278841972\n",
      "Test Precision:  0.9148718010513086\n"
     ]
    }
   ],
   "source": [
    "cov_rf_trials_metrics = []\n",
    "cov_rf_trials = train(Classifiers['Random_Forest'],COV_TYPE,cov_rf_trials_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-justice",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "coupled-joining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  1 params:  {'C': 1, 'max_iter': 10000, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Trial  1 : train accuracy:  0.9762 , f score:  0.7961909283383326 , precision:  0.8639307294712292\n",
      "Trial  2 params:  {'C': 1000, 'max_iter': 10000, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Trial  2 : train accuracy:  0.978 , f score:  0.8344203387217972 , precision:  0.8631280096622904\n",
      "Trial  3 params:  {'C': 1000, 'max_iter': 10000, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Trial  3 : train accuracy:  0.976 , f score:  0.8242882796768309 , precision:  0.8713063659411162\n",
      "Trial  4 params:  {'C': 100, 'max_iter': 10000, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Trial  4 : train accuracy:  0.9818 , f score:  0.8322369743071849 , precision:  0.885006830601093\n",
      "Trial  5 params:  {'C': 10, 'max_iter': 10000, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Trial  5 : train accuracy:  0.9794 , f score:  0.8258355244802542 , precision:  0.8751805233933418\n",
      "Test Prams:  {'C': 10, 'max_iter': 10000, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Test accuracy:  0.9762764947197189\n",
      "Test F score:  0.8074638352570408\n",
      "Test Precision:  0.8443311006508354\n"
     ]
    }
   ],
   "source": [
    "cov_logreg_trials_metrics = []\n",
    "cov_logreg_trials = train(Classifiers['Logistic_Regression'],COV_TYPE,cov_logreg_trials_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-bearing",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "indirect-surface",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  1 params:  {'alpha': 1e-08, 'fit_prior': True}\n",
      "Trial  1 : train accuracy:  0.9774 , f score:  0.8254974480932122 , precision:  0.8137852437229376\n",
      "Trial  2 params:  {'alpha': 1e-08, 'fit_prior': True}\n",
      "Trial  2 : train accuracy:  0.9752 , f score:  0.8045653533458412 , precision:  0.8343453712581229\n",
      "Trial  3 params:  {'alpha': 1e-08, 'fit_prior': True}\n",
      "Trial  3 : train accuracy:  0.9728 , f score:  0.7804752066115701 , precision:  0.7856634633215449\n",
      "Trial  4 params:  {'alpha': 1e-08, 'fit_prior': True}\n",
      "Trial  4 : train accuracy:  0.9744 , f score:  0.8051394470831812 , precision:  0.7903851747302015\n",
      "Trial  5 params:  {'alpha': 1, 'fit_prior': True}\n",
      "Trial  5 : train accuracy:  0.979 , f score:  0.8224536900041427 , precision:  0.8579719346592998\n",
      "Test Prams:  {'alpha': 1, 'fit_prior': True}\n",
      "Test accuracy:  0.9739536224134608\n",
      "Test F score:  0.7824757423542283\n",
      "Test Precision:  0.8286029671837536\n"
     ]
    }
   ],
   "source": [
    "cov_bayes_trials_metrics = []\n",
    "cov_bayes_trials = train(Classifiers['Naive_Bayes'],COV_TYPE,cov_bayes_trials_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-fashion",
   "metadata": {},
   "source": [
    "# Training KR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-reform",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "international-satellite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  1 params:  {'criterion': 'entropy', 'max_features': 2, 'n_estimators': 4}\n",
      "Trial  1 : train accuracy:  0.9902 , f score:  0.8266959650520455 , precision:  0.875683986795098\n",
      "Trial  2 params:  {'criterion': 'gini', 'max_features': 1, 'n_estimators': 16}\n",
      "Trial  2 : train accuracy:  0.9904 , f score:  0.8406565656565657 , precision:  0.8682599825123871\n",
      "Trial  3 params:  {'criterion': 'entropy', 'max_features': 2, 'n_estimators': 16}\n",
      "Trial  3 : train accuracy:  0.9898 , f score:  0.8661935079845527 , precision:  0.8938288112615372\n",
      "Trial  4 params:  {'criterion': 'gini', 'max_features': 8, 'n_estimators': 32}\n",
      "Trial  4 : train accuracy:  0.9902 , f score:  0.8291938997821351 , precision:  0.8774939172749391\n",
      "Trial  5 params:  {'criterion': 'entropy', 'max_features': 2, 'n_estimators': 16}\n",
      "Trial  5 : train accuracy:  0.9902 , f score:  0.8029919248338421 , precision:  0.8911083368012207\n",
      "Test Prams:  {'criterion': 'entropy', 'max_features': 2, 'n_estimators': 16}\n",
      "Test accuracy:  0.9849776023890785\n",
      "Test F score:  0.7109034176911072\n",
      "Test Precision:  0.7338868276981535\n"
     ]
    }
   ],
   "source": [
    "kr_rf_trials_metrics = []\n",
    "kr_rf_trials = train(Classifiers['Random_Forest'],KR,kr_rf_trials_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-chest",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "official-receptor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  1 params:  {'C': 0.0001, 'max_iter': 10000, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Trial  1 : train accuracy:  0.9876 , f score:  0.649671052631579 , precision:  0.6343244653103808\n",
      "Trial  2 params:  {'C': 0.0001, 'max_iter': 10000, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Trial  2 : train accuracy:  0.9876 , f score:  0.648664343786295 , precision:  0.6325068870523416\n",
      "Trial  3 params:  {'C': 0.0001, 'max_iter': 10000, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Trial  3 : train accuracy:  0.987 , f score:  0.6471647164716471 , precision:  0.6298185941043084\n",
      "Trial  4 params:  {'C': 0.0001, 'max_iter': 10000, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Trial  4 : train accuracy:  0.9888 , f score:  0.6497890295358649 , precision:  0.6345381526104418\n",
      "Trial  5 params:  {'C': 0.0001, 'max_iter': 10000, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Trial  5 : train accuracy:  0.9866 , f score:  0.6470243330401642 , precision:  0.6295681063122923\n",
      "Test Prams:  {'C': 0.0001, 'max_iter': 10000, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Test accuracy:  0.9874535117342795\n",
      "Test F score:  0.6490541128463628\n",
      "Test Precision:  0.6332093646247453\n"
     ]
    }
   ],
   "source": [
    "kr_logreg_trials_metrics = []\n",
    "kr_logreg_trials = train(Classifiers['Logistic_Regression'],KR,kr_logreg_trials_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-captain",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "primary-vancouver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  1 params:  {'alpha': 1e-08, 'fit_prior': True}\n",
      "Trial  1 : train accuracy:  0.9854 , f score:  0.645743766122098 , precision:  0.627292340884574\n",
      "Trial  2 params:  {'alpha': 1e-08, 'fit_prior': True}\n",
      "Trial  2 : train accuracy:  0.9882 , f score:  0.6494604841061534 , precision:  0.6339434276206323\n",
      "Trial  3 params:  {'alpha': 1e-08, 'fit_prior': True}\n",
      "Trial  3 : train accuracy:  0.9876 , f score:  0.649357900614182 , precision:  0.6337579617834396\n",
      "Trial  4 params:  {'alpha': 1e-08, 'fit_prior': True}\n",
      "Trial  4 : train accuracy:  0.9878 , f score:  0.6496797549429129 , precision:  0.6343402225755167\n",
      "Trial  5 params:  {'alpha': 1e-08, 'fit_prior': True}\n",
      "Trial  5 : train accuracy:  0.9876 , f score:  0.648048048048048 , precision:  0.6313993174061433\n",
      "Test Prams:  {'alpha': 1e-08, 'fit_prior': True}\n",
      "Test accuracy:  0.987597660419246\n",
      "Test F score:  0.6492670290343351\n",
      "Test Precision:  0.6335937611179895\n"
     ]
    }
   ],
   "source": [
    "kr_bayes_trials_metrics = []\n",
    "kr_bayes_trials = train(Classifiers['Naive_Bayes'],KR,kr_bayes_trials_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
